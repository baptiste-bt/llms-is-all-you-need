# LLMs Architectures

## Starting papers

1. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
2. [GPT-3: Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) - OpenAI


## Large language models evolution per year

### LLMs in 2020

1. [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf) - OpenAI

### LLMs in 2021

1. [Jurassic-1: Technical details and evaluation](https://sharir.org/papers/jurassic_white_paper.pdf)

### LLMs in 2022

1. [Chinchilla: Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556)

### LLMs in 2023

1. [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)
2. [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf)

## Mixture of Experts

1. [GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/abs/2112.06905)
2. [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity - 2022](https://www.jmlr.org/papers/v23/21-0998.html)
3. [Designing effective sparse expert models - 2022](https://www.thetalkingmachines.com/sites/default/files/2022-03/2202.08906.pdf)
4. [Unified Scaling Laws for Routed Language Models](https://arxiv.org/abs/2202.01169.pdf)

## Hyperparameters

1. [Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](https://openreview.net/forum?id=Bx6qKuBM2AD): Setting learning rate and batch size