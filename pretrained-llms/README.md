# Pre-trained LLMs

## List of pretrained model and sizes

## Pre-training datasets commonly used

1. English CommonCrawl
2. C4
3. Github
4. Wikipedia
5. Gutenberg and Books3
6. ArXiv
7. Stack Exchange

## Data pre-processing steps

1. [CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data](https://aclanthology.org/2020.lrec-1.494.pdf)

## Model and data parallelism techniques

1. DeepSpeed
2. Fully Sharded Data Parallelism (FSDP)

## Other improvements

1. FlashAttention
2. Meta `xformers` -> efficient forward and backward implementation of attention